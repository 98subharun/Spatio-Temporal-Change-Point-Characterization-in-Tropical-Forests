{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM-yWtRdcacR"
      },
      "source": [
        "##Hello!!\n",
        "This is the actual code for the full hybrid model consisting of everything that I have stated, pls note that running this will actually take some time\n",
        "(also making this took much more time than i hoped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwtl7WuaebF"
      },
      "outputs": [],
      "source": [
      "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from scipy.signal import find_peaks, savgol_filter\n",
        "from scipy import stats\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_RIivecc0b1",
        "outputId": "229d9c60-b9f2-43b4-8349-b4dc20400846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hybrid ai forest chnage detction system\n",
            "generated 2025-09-19 09:22:55\n",
            "huber Regression:  yes\n",
            "multi index validation: yes\n",
            "savitzkygolay smoothing:  yes\n",
            "probability calibration:  yes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#ablation study flgs for experimenting such that you can test out the differences yourself;\n",
        "USE_HUBER = True       #Using Huber regression vs ols\n",
        "USE_MULTIINDEX = True  #Use multi index validation vs NDVI only\n",
        "USE_SAVGOL = True      #Use savitzky golay vs gausian smoothing\n",
        "USE_CALIBRATION = True #probabiltiies for reliability\n",
        "\n",
        "print(\"hybrid ai forest chnage detction system\")\n",
        "\n",
        "print(\"generated\", pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "print(f\"huber Regression: {' yes' if USE_HUBER else 'no (using OLS)'}\")\n",
        "print(f\"multi index validation: {'yes' if USE_MULTIINDEX else ' no (using ndvi only)'}\")\n",
        "print(f\"savitzkygolay smoothing: {' yes' if USE_SAVGOL else 'no (using Gaussian)'}\")\n",
        "print(f\"probability calibration: {' yes' if USE_CALIBRATION else ' no'}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De0b427zenZ3"
      },
      "outputs": [],
      "source": [
        "#fun fact...this took wayyyy too long\n",
        "class ForestChangeDetectionSystem:\n",
        "    def __init__(self, study_area=\"Amazon Tropical Forest\", patch_size=64):\n",
        "        self.study_area = study_area\n",
        "        self.patch_size = patch_size\n",
        "        self.change_classes = [\n",
        "            'Stable Forest', 'Minor Degradation', 'Moderate Deforestation',\n",
        "            'Major Deforestation', 'Regrowth/Recovery'\n",
        "        ]\n",
        "        self.min_forest_ndvi = 0.4 #min ndvi for what counts as forest\n",
        "        self.forest_change_thresholds = { #these are the values for figuring out what kind of change happened\n",
        "            'fire_scar': 0.6,\n",
        "            'major_deforestation': 0.4,\n",
        "            'moderate_deforestation': 0.25,\n",
        "            'minor_degradation': 0.1,\n",
        "            'regrowth': -0.15,\n",
        "            'seasonal_variation': 0.05 #small changes probably just seasons\n",
        "        }\n",
        "        self.enhanced_time_series_features = None\n",
        "        self.cnn_spatial_features = None\n",
        "        self.ancillary_features = None\n",
        "        self.gbm_model = None #holds the trained gradient boosting model later\n",
        "        self.feature_selector = None #for picking the best features\n",
        "        self.label_encoder = LabelEncoder() #convert class names to numbers\n",
        "        self.feature_scaler = RobustScaler() #scale features so model works better\n",
        "        self.model_metrics = {} #stores how well the model did\n",
        "\n",
        "    def simulate_satellite_data(self, n_pixels=7500, n_timesteps=48):\n",
        "        print(\"generating satellite time series data\")\n",
        "\n",
        "        time_series_ndvi = []\n",
        "        time_series_nbr = []\n",
        "        time_series_ndmi = []\n",
        "        time_series_evi = []\n",
        "        labels = []\n",
        "        pixel_coordinates = []\n",
        "        change_metadata = []\n",
        "        spatial_tiles = [] #gonna use this for spatial split later\n",
        "        for pixel_id in range(n_pixels):\n",
        "            if pixel_id % 1000 == 0:\n",
        "                print(f\"generating pixel {pixel_id}/{n_pixels}...\")\n",
        "            row = np.random.randint(0, 1500)\n",
        "            col = np.random.randint(0, 1500)\n",
        "            pixel_coordinates.append((row, col))\n",
        "            tile_row = row // 375  #a 4x4 grid of tiles, this helps w/ spatial bias\n",
        "            tile_col = col // 375\n",
        "            tile_id = tile_row * 4 + tile_col #unique ID for each tile\n",
        "            spatial_tiles.append(tile_id)\n",
        "\n",
        "            t = np.arange(n_timesteps)\n",
        "\n",
        "            #simulating seasonal patterns using harmonics - kinda like real satellite data\n",
        "            seasonal_primary = 0.09 * np.sin(2 * np.pi * t / 12 + np.random.uniform(0, np.pi/2))\n",
        "            seasonal_secondary = 0.05 * np.sin(4 * np.pi * t / 12 + np.random.uniform(0, np.pi/4))\n",
        "            seasonal_tertiary = 0.03 * np.sin(6 * np.pi * t / 12 + np.random.uniform(0, np.pi/6))\n",
        "            seasonal_quaternary = 0.02 * np.sin(8 * np.pi * t / 12 + np.random.uniform(0, np.pi/8)) #added this fourth one for fun\n",
        "            seasonal_pattern = seasonal_primary + seasonal_secondary + seasonal_tertiary + seasonal_quaternary\n",
        "\n",
        "            #Base index values with some noise\n",
        "            base_ndvi = 0.75 + seasonal_pattern + np.random.normal(0, 0.025, n_timesteps) #typical forest NDVI\n",
        "            base_nbr = 0.68 + 0.85 * seasonal_pattern + np.random.normal(0, 0.02, n_timesteps) #NBR usually tracks NDVI\n",
        "            base_ndmi = 0.48 + 0.7 * seasonal_pattern + np.random.normal(0, 0.015, n_timesteps) #NDMI for water content\n",
        "            base_evi = 0.52 + 0.9 * seasonal_pattern + np.random.normal(0, 0.018, n_timesteps) #EVI is another vegetation index\n",
        "\n",
        "            #Deciding if a change happens and what kind\n",
        "            #probabilities for different change types\n",
        "            change_probabilities = [0.32, 0.22, 0.18, 0.16, 0.12] #probs: Stable, Minor, Moderate, Major, Regrowth\n",
        "            change_type_idx = np.random.choice(5, p=change_probabilities)\n",
        "            change_type = self.change_classes[change_type_idx]\n",
        "\n",
        "            ndvi_series = base_ndvi.copy()\n",
        "            nbr_series = base_nbr.copy()\n",
        "            ndmi_series = base_ndmi.copy()\n",
        "            evi_series = base_evi.copy()\n",
        "\n",
        "            change_happened = False\n",
        "            change_time = -1\n",
        "            change_magnitude = 0\n",
        "            change_persistence = 0\n",
        "\n",
        "            if change_type != 'Stable Forest':\n",
        "                change_happened = True\n",
        "                #picking a random time for the change to happen, not too early or late\n",
        "                change_time = np.random.randint(5, n_timesteps - 10) #avoiding edges\n",
        "\n",
        "                #appling the change based on type\n",
        "                if change_type == 'Minor Degradation':\n",
        "                    mag_factor = np.random.uniform(-0.08, -0.15)\n",
        "                    persist_factor = np.random.randint(3, 8)\n",
        "                elif change_type == 'Moderate Deforestation':\n",
        "                    mag_factor = np.random.uniform(-0.16, -0.3)\n",
        "                    persist_factor = np.random.randint(6, 12)\n",
        "                elif change_type == 'Major Deforestation':\n",
        "                    mag_factor = np.random.uniform(-0.31, -0.6)\n",
        "                    persist_factor = np.random.randint(10, 20)\n",
        "                elif change_type == 'Fire Scar': # typo in comment\n",
        "                    mag_factor = np.random.uniform(-0.5, -0.8) #big drop for fire\n",
        "                    persist_factor = np.random.randint(8, 15)\n",
        "                elif change_type == 'Regrowth/Recovery':\n",
        "                    mag_factor = np.random.uniform(0.05, 0.2) #positive change for regrowth\n",
        "\n",
        "                    #ensuring upper bound is greater than lower bound\n",
        "                    upper_bound = n_timesteps - change_time\n",
        "                    if upper_bound > 15:\n",
        "                        persist_factor = np.random.randint(15, upper_bound) #can be long recovery\n",
        "                    else:\n",
        "                        persist_factor = 15 #Default to 15 if upper bound is not large enough\n",
        "\n",
        "\n",
        "                change_magnitude = mag_factor * np.random.uniform(0.8, 1.2) #add some variability\n",
        "                change_persistence = persist_factor\n",
        "\n",
        "                #apply the change effect gradually over a few timesteps\n",
        "                effect_duration = min(3, n_timesteps - change_time -1 ) #how long the drop/increase takes\n",
        "                recovery_duration = min(change_persistence, n_timesteps - change_time - effect_duration -1) #how long it stays changed or recovers\n",
        "\n",
        "                #initial drop/increase\n",
        "                for i in range(effect_duration):\n",
        "                    step_mag = (change_magnitude / effect_duration) * (i + 1)\n",
        "                    ndvi_series[change_time + i + 1:] += step_mag\n",
        "                    nbr_series[change_time + i + 1:] += step_mag * 0.9 #NBR slightly less sensitive\n",
        "                    ndmi_series[change_time + i + 1:] += step_mag * 0.7 #NDMI even less\n",
        "                    evi_series[change_time + i + 1:] += step_mag * 1.1 #EVI maybe more sensitive\n",
        "\n",
        "                #maintain or recover\n",
        "                if change_type in ['Minor Degradation', 'Moderate Deforestation', 'Major Deforestation', 'Fire Scar']:\n",
        "                    #For degradation/deforestation, maintaining the low value with some noise\n",
        "                    for i in range(recovery_duration):\n",
        "                         ndvi_series[change_time + effect_duration + i + 1] += np.random.normal(0, 0.01) #small fluctuations\n",
        "                         nbr_series[change_time + effect_duration + i + 1] += np.random.normal(0, 0.008)\n",
        "                         ndmi_series[change_time + effect_duration + i + 1] += np.random.normal(0, 0.005)\n",
        "                         evi_series[change_time + effect_duration + i + 1] += np.random.normal(0, 0.009)\n",
        "\n",
        "                elif change_type == 'Regrowth/Recovery':\n",
        "                    #for regrowth, simulating a recovery trend\n",
        "                    recovery_amount = abs(change_magnitude) * np.random.uniform(0.5, 1.0) #recover part or all\n",
        "                    for i in range(recovery_duration):\n",
        "                        recovery_step = (recovery_amount / recovery_duration) * (i + 1)\n",
        "                        ndvi_series[change_time + effect_duration + i + 1:] += recovery_step #gradual recovery\n",
        "                        nbr_series[change_time + effect_duration + i + 1:] += recovery_step * 0.9\n",
        "                        ndmi_series[change_time + effect_duration + i + 1:] += recovery_step * 0.75\n",
        "                        evi_series[change_time + effect_duration + i + 1:] += recovery_step * 1.05\n",
        "\n",
        "            #adding some general noise to the whole series(for good measure)\n",
        "            ndvi_series += np.random.normal(0, 0.01, n_timesteps)\n",
        "            nbr_series += np.random.normal(0, 0.008, n_timesteps)\n",
        "            ndmi_series += np.random.normal(0, 0.005, n_timesteps)\n",
        "            evi_series += np.random.normal(0, 0.009, n_timesteps)\n",
        "\n",
        "            #make sure values are within a realistic range (0 to 1)\n",
        "            ndvi_series = np.clip(ndvi_series, 0, 1)\n",
        "            nbr_series = np.clip(nbr_series, 0, 1)\n",
        "            ndmi_series = np.clip(ndmi_series, 0, 1)\n",
        "            evi_series = np.clip(evi_series, 0, 1)\n",
        "\n",
        "\n",
        "            time_series_ndvi.append(ndvi_series)\n",
        "            time_series_nbr.append(nbr_series)\n",
        "            time_series_ndmi.append(ndmi_series)\n",
        "            time_series_evi.append(evi_series)\n",
        "            labels.append(change_type)\n",
        "            change_metadata.append({\n",
        "                'pixel_id': pixel_id,\n",
        "                'row': row, 'col': col,\n",
        "                'change_happened': change_happened,\n",
        "                'change_type': change_type,\n",
        "                'change_time': change_time,\n",
        "                'change_magnitude': change_magnitude,\n",
        "                'change_persistence': change_persistence\n",
        "            })\n",
        "\n",
        "        self.time_series_ndvi = np.array(time_series_ndvi)\n",
        "        self.time_series_nbr = np.array(time_series_nbr)\n",
        "        self.time_series_ndmi = np.array(time_series_ndmi)\n",
        "        self.time_series_evi = np.array(time_series_evi)\n",
        "        self.labels = np.array(labels)\n",
        "        self.pixel_coordinates = np.array(pixel_coordinates)\n",
        "        self.change_metadata = change_metadata\n",
        "        self.spatial_tiles = np.array(spatial_tiles)\n",
        "\n",
        "        print(f\"Generated {n_pixels} pixel time series.\")\n",
        "        print(f\"time steps {n_timesteps}\")\n",
        "        print(f\"change types {np.unique(labels)}\")\n",
        "        print(f\"example labels {labels[:10]}\") #showing a few examples\n",
        "        print(f\"spatial tiles generated {len(np.unique(spatial_tiles))}\") #how many tiles?\n",
        "\n",
        "        return self.time_series_ndvi, self.labels\n",
        "\n",
        "\n",
        "    def perform_bfast_analysis(self):\n",
        "        print(\"\\nstage 2 Performing bfast analysis\")\n",
        "\n",
        "        change_candidates = []\n",
        "        enhanced_ts_features = []\n",
        "        n_pixels = len(self.time_series_ndvi)\n",
        "        for pixel_idx in range(n_pixels):\n",
        "            if pixel_idx % 1000 == 0:\n",
        "                print(f\"Processing pixel {pixel_idx}/{n_pixels}...\")\n",
        "            ndvi_ts = self.time_series_ndvi[pixel_idx]\n",
        "            nbr_ts = self.time_series_nbr[pixel_idx]\n",
        "            ndmi_ts = self.time_series_ndmi[pixel_idx]\n",
        "            evi_ts = self.time_series_evi[pixel_idx]\n",
        "            t_normalized = np.arange(len(ndvi_ts)) / 12.0\n",
        "            X_design = np.column_stack([\n",
        "                np.ones(len(ndvi_ts)),\n",
        "                t_normalized,\n",
        "                np.sin(2 * np.pi * t_normalized), np.cos(2 * np.pi * t_normalized),\n",
        "                np.sin(4 * np.pi * t_normalized), np.cos(4 * np.pi * t_normalized),\n",
        "                np.sin(6 * np.pi * t_normalized), np.cos(6 * np.pi * t_normalized),\n",
        "                np.sin(8 * np.pi * t_normalized), np.cos(8 * np.pi * t_normalized)\n",
        "            ])\n",
        "            regression_model = HuberRegressor(epsilon=1.35, alpha=0.01, max_iter=200) if USE_HUBER else LinearRegression()\n",
        "            regression_model.fit(X_design, ndvi_ts)\n",
        "            predicted_ndvi = regression_model.predict(X_design)\n",
        "            residuals = ndvi_ts - predicted_ndvi\n",
        "            forest_density = np.median(ndvi_ts)\n",
        "            if forest_density > 0.7:\n",
        "                threshold_multiplier = 2.2 #high density forest\n",
        "            elif forest_density > 0.5:\n",
        "                threshold_multiplier = 2.8 #medium density forest\n",
        "            elif forest_density > 0.3:\n",
        "                threshold_multiplier = 3.5 #low density forest\n",
        "            else:\n",
        "                threshold_multiplier = 4.0 #sparse areas\n",
        "            residual_std = np.std(residuals)\n",
        "            change_threshold = threshold_multiplier * residual_std\n",
        "            smoothed_residuals = (savgol_filter(residuals, window_length=5, polyorder=2)\n",
        "                                  if USE_SAVGOL else gaussian_filter1d(residuals, sigma=1.0))\n",
        "            peaks, _ = find_peaks(\n",
        "                np.abs(smoothed_residuals),\n",
        "                height=change_threshold,\n",
        "                distance=4,\n",
        "                prominence=change_threshold * 0.6,\n",
        "                width=2\n",
        "            )\n",
        "            validated_changes = []\n",
        "            for peak_idx in peaks:\n",
        "                if 4 <= peak_idx <= len(ndvi_ts) - 4:\n",
        "                    change_analysis = self._analyze_potential_change(\n",
        "                        peak_idx, ndvi_ts, nbr_ts, ndmi_ts, evi_ts, residuals\n",
        "                    )\n",
        "                    if change_analysis['is_significant']:\n",
        "                        validated_changes.append(change_analysis)\n",
        "            if len(validated_changes) > 0:\n",
        "                change_candidates.append(pixel_idx)\n",
        "            ts_features = self._extract_time_series_features(\n",
        "                ndvi_ts, nbr_ts, ndmi_ts, evi_ts, validated_changes,\n",
        "                residuals, predicted_ndvi, regression_model.coef_\n",
        "            )\n",
        "            enhanced_ts_features.append(ts_features)\n",
        "        self.enhanced_time_series_features = np.array(enhanced_ts_features)\n",
        "        self.change_candidates = change_candidates\n",
        "        print(f\"bfast analysis complete. Found {len(change_candidates)} change candidates.\")\n",
        "        print(f\"extracted {self.enhanced_time_series_features.shape[1]} time series feature(per pixel)\")\n",
        "        return change_candidates\n",
        "\n",
        "    def _analyze_potential_change(self, change_idx, ndvi_ts, nbr_ts, ndmi_ts, evi_ts, residuals):\n",
        "        n_obs = len(ndvi_ts)\n",
        "        window_size = min(8, change_idx, n_obs - change_idx) #window size before and after\n",
        "        before_start = max(0, change_idx - window_size)\n",
        "        after_end = min(n_obs, change_idx + window_size)\n",
        "        indices = {'ndvi': ndvi_ts, 'nbr': nbr_ts, 'ndmi': ndmi_ts, 'evi': evi_ts} if USE_MULTIINDEX else {'ndvi': ndvi_ts}\n",
        "        index_changes = {}\n",
        "        for index_name, index_ts in indices.items():\n",
        "            pre_values = index_ts[before_start:change_idx]\n",
        "            post_values = index_ts[change_idx:after_end]\n",
        "            if len(pre_values) > 2 and len(post_values) > 2:\n",
        "                magnitude = np.mean(post_values) - np.mean(pre_values) #bascally how big was the change?\n",
        "                t_stat, p_value = stats.ttest_ind(pre_values, post_values, equal_var=False) #doing a t-test\n",
        "                index_changes[index_name] = {\n",
        "                    'magnitude': magnitude,\n",
        "                    'p_value': p_value,\n",
        "                    'significant': p_value < 0.05 and abs(magnitude) > 0.03 #statistically significant?\n",
        "                }\n",
        "            else:\n",
        "                index_changes[index_name] = {\n",
        "                    'magnitude': 0, 'p_value': 1.0, 'significant': False # not enuough data for test\n",
        "                }\n",
        "        ndvi_change = index_changes['ndvi']\n",
        "        validation_score = 1.0                                                                       #base score\n",
        "        if USE_MULTIINDEX:\n",
        "            for other_index in ['nbr', 'ndmi', 'evi']:\n",
        "                if other_index in index_changes:\n",
        "                    other_change = index_changes[other_index]\n",
        "                    same_direction = np.sign(ndvi_change['magnitude']) == np.sign(other_change['magnitude']) #change in same direction?\n",
        "                    both_significant = ndvi_change['significant'] and other_change['significant']             #both changes significant?\n",
        "                    if same_direction and both_significant:\n",
        "                        validation_score += 0.4 #good sign\n",
        "                    elif same_direction:\n",
        "                        validation_score += 0.2 #okay sign\n",
        "                    else:\n",
        "                        validation_score -= 0.3 #bad sign, maybe not real change\n",
        "        primary_magnitude = ndvi_change['magnitude']\n",
        "        abs_magnitude = abs(primary_magnitude)\n",
        "        if primary_magnitude < 0:                                      #if NDVI dropped\n",
        "            if abs_magnitude > self.forest_change_thresholds['fire_scar']:\n",
        "                change_type = 'fire_scar'\n",
        "            elif abs_magnitude > self.forest_change_thresholds['major_deforestation']:\n",
        "                change_type = 'major_deforestation'\n",
        "            elif abs_magnitude > self.forest_change_thresholds['moderate_deforestation']:\n",
        "                change_type = 'moderate_deforestation'\n",
        "            elif abs_magnitude > self.forest_change_thresholds['minor_degradation']:\n",
        "                change_type = 'minor_degradation'\n",
        "            else:\n",
        "                change_type = 'seasonal_variation' #just seasonal?\n",
        "        else:                                                        #if NDVI increased\n",
        "            if abs_magnitude > abs(self.forest_change_thresholds['regrowth']):\n",
        "                change_type = 'regrowth' #recovery or growth?\n",
        "            else:\n",
        "                change_type = 'seasonal_variation' #probably seasonal\n",
        "        persistence = self._calculate_change_persistence(change_idx, ndvi_ts, primary_magnitude) #how long did it last?\n",
        "        is_significant = (                                                                        #putting it all together\n",
        "            validation_score > 1.0 and\n",
        "            abs_magnitude > 0.05 and                                                 #needs to be noticeable\n",
        "            persistence >= 2 and                                                    #needs to last more than one timestep(obviously)\n",
        "            change_type != 'seasonal_variation' #not just seasonal\n",
        "        )\n",
        "        return {\n",
        "            'index': change_idx,\n",
        "            'magnitude': primary_magnitude,\n",
        "            'change_type': change_type,\n",
        "            'validation_score': min(validation_score, 4.0), #cap score\n",
        "            'persistence': persistence,\n",
        "            'is_significant': is_significant,\n",
        "            'confidence': 1 - ndvi_change['p_value'], #confidence from p-value\n",
        "            'multi_index_changes': index_changes\n",
        "        }\n",
        "\n",
        "    def _calculate_change_persistence(self, change_idx, ndvi_ts, magnitude):\n",
        "        n_obs = len(ndvi_ts)\n",
        "        persistence = 1\n",
        "        threshold = abs(magnitude) * 0.4 #how much change needs to be maintained\n",
        "        baseline_value = ndvi_ts[change_idx - 1] if change_idx > 0 else ndvi_ts[0] #value before change\n",
        "        for i in range(change_idx + 1, min(change_idx + 15, n_obs)):\n",
        "            current_change = abs(ndvi_ts[i] - baseline_value)\n",
        "            if current_change > threshold:\n",
        "                persistence += 1\n",
        "            else:\n",
        "                break #didnt persist\n",
        "        return persistence\n",
        "\n",
        "    def _extract_time_series_features(self, ndvi_ts, nbr_ts, ndmi_ts, evi_ts, validated_changes, residuals, predicted, coeffs):\n",
        "        features = []\n",
        "        for index_ts in [ndvi_ts, nbr_ts, ndmi_ts, evi_ts]: #eatures for each index time series\n",
        "            features.extend([np.mean(index_ts), np.std(index_ts), np.min(index_ts), np.max(index_ts)])\n",
        "        trend_slope = coeffs[1]                                                                              #slope from regression\n",
        "        trend_curvature = np.polyfit(range(len(ndvi_ts)), ndvi_ts, 2)[0] #how trend bends\n",
        "        features.extend([trend_slope, abs(trend_slope), trend_curvature])\n",
        "        seasonal_1_amplitude = np.sqrt(coeffs[2]**2 + coeffs[3]**2)                                  #amplitude of 1st harmonic\n",
        "        seasonal_2_amplitude = np.sqrt(coeffs[4]**2 + coeffs[5]**2)                                #amplitude of 2nd harmonic\n",
        "        seasonal_3_amplitude = np.sqrt(coeffs[6]**2 + coeffs[7]**2)                              #amplitude of 3rd harmonic\n",
        "        seasonal_4_amplitude = np.sqrt(coeffs[8]**2 + coeffs[9]**2)                            #amplitude of 4th harmonic\n",
        "        total_seasonal_amplitude = seasonal_1_amplitude + seasonal_2_amplitude + seasonal_3_amplitude + seasonal_4_amplitude           #sum\n",
        "        features.extend([seasonal_1_amplitude, seasonal_2_amplitude, seasonal_3_amplitude, seasonal_4_amplitude, total_seasonal_amplitude])\n",
        "        forest_density_score = np.mean(ndvi_ts > self.min_forest_ndvi)                                                           #proportion of timesteps as forest\n",
        "        vegetation_vigor = np.mean(ndvi_ts[ndvi_ts > 0.3]) if np.any(ndvi_ts > 0.3) else 0                                    #avg ndvi during growing season\n",
        "        features.extend([forest_density_score, vegetation_vigor])\n",
        "        n_changes = len(validated_changes)                                                                                #number of significant changes found\n",
        "        change_frequency = n_changes / len(ndvi_ts)                                                                   #how often changes happen\n",
        "        features.extend([n_changes, change_frequency])\n",
        "        if validated_changes:                                                                                                #features related to changes found\n",
        "            magnitudes = [change['magnitude'] for change in validated_changes]\n",
        "            max_change_magnitude = max(magnitudes, key=abs)                                                          #biggest change\n",
        "            mean_validation_score = np.mean([change['validation_score'] for change in validated_changes]) #avg validation score\n",
        "            mean_persistence = np.mean([change['persistence'] for change in validated_changes])              #avg persistence\n",
        "            change_diversity = len(set([change['change_type'] for change in validated_changes]))    #types of changes found\n",
        "            features.extend([max_change_magnitude, mean_validation_score, mean_persistence, change_diversity])\n",
        "        else:\n",
        "            features.extend([0, 0, 0, 0])\n",
        "        features.extend([\n",
        "            np.std(residuals), np.max(np.abs(residuals)),\n",
        "            stats.skew(residuals), stats.kurtosis(residuals)\n",
        "        ])\n",
        "        return features\n",
        "\n",
        "    def extract_cnn_spatial_features(self):\n",
        "        print(\"\\nStage 3 Extracting cnn Spatial Features (placeholder)\")\n",
        "\n",
        "        print(\"here I am simulating CNN features for demo.\")\n",
        "        print(\"in an actual use case this would most likely be replace with real cnn embeddings.\")\n",
        "        n_cnn_features = 512\n",
        "        cnn_features = []\n",
        "        n_pixels = len(self.time_series_ndvi)\n",
        "        for pixel_idx in range(n_pixels):\n",
        "            if pixel_idx % 1000 == 0:\n",
        "                print(f\"Generating features for pixel {pixel_idx}/{n_pixels}...\")\n",
        "            row, col = self.pixel_coordinates[pixel_idx]\n",
        "            ndvi_ts = self.time_series_ndvi[pixel_idx]\n",
        "            spatial_features = np.random.normal(0, 1, n_cnn_features) #random features\n",
        "            ts_variance = np.var(ndvi_ts)                             #variance over t\n",
        "            ts_trend = np.polyfit(range(len(ndvi_ts)), ndvi_ts, 1)[0] #trend over t\n",
        "            ts_mean = np.mean(ndvi_ts)                                 #average ndvi\n",
        "            ts_range = np.max(ndvi_ts) - np.min(ndvi_ts)               #rang of ndvi\n",
        "            spatial_features[:100] += ts_variance * 5 + np.random.normal(0, 0.1, 100) #injecting some time series info\n",
        "            spatial_features[100:200] += ts_trend * 10 + np.random.normal(0, 0.1, 100)\n",
        "            spatial_features[200:300] += (ts_mean - 0.5) * 3 + np.random.normal(0, 0.1, 100)\n",
        "            spatial_features[300:400] += ts_range * 4 + np.random.normal(0, 0.1, 100)\n",
        "            spatial_features[400:450] += 0.3 * np.sin(row / 50) + 0.2 * np.cos(col / 50) #inject spatial info\n",
        "            spatial_features[450:500] += 0.2 * (row % 100) / 100 + 0.2 * (col % 100) / 100\n",
        "            center_distance = np.sqrt((row - 750)**2 + (col - 750)**2)                    #dist from center\n",
        "            edge_proximity = min(row, col, 1500-row, 1500-col) / 1500                     #dist to nearest edge\n",
        "            spatial_features[500:512] += (0.15 * np.sin(center_distance / 100) +\n",
        "                                          0.1 * edge_proximity +\n",
        "                                          np.random.normal(0, 0.05, 12))                  #inject more spatial info\n",
        "            cnn_features.append(spatial_features)\n",
        "        self.cnn_spatial_features = np.array(cnn_features)\n",
        "        print(f\"CNN feature simulation complete.\")\n",
        "        print(f\"Extracted {n_cnn_features} features per pixel.\")\n",
        "        return self.cnn_spatial_features\n",
        "\n",
        "    def extract_ancillary_features(self):\n",
        "        print(\"\\nStage 4: Extracting Ancillary Features\")\n",
        "\n",
        "        ancillary_features = []\n",
        "        for pixel_idx in range(len(self.time_series_ndvi)):\n",
        "            row, col = self.pixel_coordinates[pixel_idx]\n",
        "            features = []\n",
        "            features.extend([row / 1500, col / 1500, (row * col) / (1500 * 1500)])                 #normalized coords\n",
        "            center_distance = np.sqrt((row - 750)**2 + (col - 750)**2) / 1061\n",
        "            edge_distance = min(row, col, 1500-row, 1500-col) / 750                                #normalized dist to edge\n",
        "            features.extend([center_distance, edge_distance])\n",
        "            features.extend([\n",
        "                np.sin(row / 100), np.cos(row / 100),\n",
        "                np.sin(col / 100), np.cos(col / 100),\n",
        "                np.sin(2 * np.pi * row / 200), np.cos(2 * np.pi * col / 200)\n",
        "            ])\n",
        "            ndvi_ts = self.time_series_ndvi[pixel_idx]\n",
        "            nbr_ts = self.time_series_nbr[pixel_idx]\n",
        "            ndmi_ts = self.time_series_ndmi[pixel_idx]\n",
        "            evi_ts = self.time_series_evi[pixel_idx]\n",
        "            try:                                               #corrs between indices\n",
        "                ndvi_nbr_corr = np.corrcoef(ndvi_ts, nbr_ts)[0, 1]\n",
        "                ndvi_ndmi_corr = np.corrcoef(ndvi_ts, ndmi_ts)[0, 1]\n",
        "                ndvi_evi_corr = np.corrcoef(ndvi_ts, evi_ts)[0, 1]\n",
        "                ndvi_nbr_corr = 0 if np.isnan(ndvi_nbr_corr) else ndvi_nbr_corr\n",
        "                ndvi_ndmi_corr = 0 if np.isnan(ndvi_ndmi_corr) else ndmi_ndmi_corr\n",
        "                ndvi_evi_corr = 0 if np.isnan(ndvi_evi_corr) else ndvi_evi_corr\n",
        "            except:                                                #handle cases where correlation cant be calc\n",
        "                ndvi_nbr_corr = ndmi_ndmi_corr = ndvi_evi_corr = 0\n",
        "            features.extend([ndvi_nbr_corr, ndvi_ndmi_corr, ndvi_evi_corr])\n",
        "            mean_ndvi = np.mean(ndvi_ts) #avg values of indices\n",
        "            mean_nbr = np.mean(nbr_ts)\n",
        "            mean_ndmi = np.mean(ndmi_ts)\n",
        "            mean_evi = np.mean(evi_ts)\n",
        "            features.extend([                                      #ratios and diffs between indices\n",
        "                mean_ndvi / (mean_nbr + 0.001),\n",
        "                mean_ndvi - mean_ndmi,\n",
        "                (mean_ndvi + mean_evi) / 2,\n",
        "                np.std([mean_ndvi, mean_nbr, mean_ndmi, mean_evi]) #std dev of mean values\n",
        "            ])\n",
        "            ancillary_features.append(features)\n",
        "        self.ancillary_features = np.array(ancillary_features)\n",
        "        print(f\"ancillary feature extraction done.\")\n",
        "        print(f\"extracted {self.ancillary_features.shape[1]} features(per pixel)\")\n",
        "        return self.ancillary_features\n",
        "\n",
        "    def create_spatial_train_test_split(self, test_size=0.25, random_state=42):\n",
        "        print(\"\\ncreating spatial train/test split...\")\n",
        "        print(f\"using a random state {random_state}\")\n",
        "        unique_tiles = np.unique(self.spatial_tiles)\n",
        "        n_tiles = len(unique_tiles)\n",
        "        print(f\"total tiles available {n_tiles}\")\n",
        "        n_test_tiles = max(1, int(n_tiles * test_size))\n",
        "        n_train_tiles = n_tiles - n_test_tiles\n",
        "        tile_random = np.random.RandomState(random_state)\n",
        "        tile_random.shuffle(unique_tiles)\n",
        "        test_tiles = unique_tiles[:n_test_tiles]\n",
        "        train_tiles = unique_tiles[n_test_tiles:]\n",
        "        train_mask = np.isin(self.spatial_tiles, train_tiles)\n",
        "        test_mask = np.isin(self.spatial_tiles, test_tiles)\n",
        "        print(f\"gave {n_train_tiles} tiles for training and {n_test_tiles} for testing.\")\n",
        "        print(f\"pixel counts {np.sum(train_mask)} training pixels, {np.sum(test_mask)} test pixels.\")\n",
        "        print(f\"test split ratio {np.sum(test_mask)/(np.sum(train_mask)+np.sum(test_mask)):.3f}\")\n",
        "        return train_mask, test_mask\n",
        "\n",
        "    def train_classifier_model(self, test_size=0.25):\n",
        "        print(\"\\nstage 5 Training gb classifier\")\n",
        "\n",
        "        all_features = np.concatenate([\n",
        "            self.cnn_spatial_features,\n",
        "            self.enhanced_time_series_features,\n",
        "            self.ancillary_features\n",
        "        ], axis=1)\n",
        "\n",
        "        #just to make sure everything is working fine till now\n",
        "        print(f\"otal features for training {all_features.shape[1]}\")\n",
        "        print(f\"cnn spatial features {self.cnn_spatial_features.shape[1]}\")\n",
        "        print(f\"time series features {self.enhanced_time_series_features.shape[1]}\")\n",
        "        print(f\"ancillary features {self.ancillary_features.shape[1]}\")\n",
        "\n",
        "        y_encoded = self.label_encoder.fit_transform(self.labels)\n",
        "        train_mask, test_mask = self.create_spatial_train_test_split(test_size, random_state=42)\n",
        "        X_train = all_features[train_mask]\n",
        "        X_test = all_features[test_mask]\n",
        "        y_train = y_encoded[train_mask]\n",
        "        y_test = y_encoded[test_mask]\n",
        "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.feature_scaler.transform(X_test)\n",
        "        print(\"\\nperforming a feature selection...\")\n",
        "        self.feature_selector = SelectKBest(score_func=f_classif, k=min(200, all_features.shape[1]))\n",
        "        X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train)\n",
        "        X_test_selected = self.feature_selector.transform(X_test_scaled)\n",
        "        print(f\"selected {X_train_selected.shape[1]} features.\")\n",
        "        base_gbm = GradientBoostingClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.08,\n",
        "            subsample=0.85,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            validation_fraction=0.15,\n",
        "            n_iter_no_change=20\n",
        "        )\n",
        "        print(\"training the gb model...\")\n",
        "        if USE_CALIBRATION:\n",
        "            print(\"applying probability calibr.\")\n",
        "            self.gbm_model = CalibratedClassifierCV(base_gbm, method='sigmoid', cv=3)\n",
        "        else:\n",
        "            self.gbm_model = base_gbm\n",
        "        self.gbm_model.fit(X_train_selected, y_train)\n",
        "        y_pred = self.gbm_model.predict(X_test_selected)\n",
        "        y_pred_proba = self.gbm_model.predict_proba(X_test_selected)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "        print(f\"\\ntraining done\")\n",
        "        print(f\"accuracy {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "        print(f\"Weighted F1-Score: {f1:.4f}\")\n",
        "        if USE_CALIBRATION:\n",
        "            print(f\"calibration applied.\")\n",
        "        self.model_metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'n_features_selected': X_train_selected.shape[1],\n",
        "            'n_training_samples': X_train.shape[0],\n",
        "            'calibrated': USE_CALIBRATION\n",
        "        }\n",
        "        class_names = self.label_encoder.classes_\n",
        "        print(f\"\\nClassification Report\")\n",
        "        print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "        self._create_confusion_matrix_plot(y_test, y_pred, class_names)\n",
        "        return X_test_selected, y_test, y_pred, class_names, y_pred_proba\n",
        "\n",
        "    def _create_confusion_matrix_plot(self, y_true, y_pred, class_names):\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        print(\"\\ncreating confusion matrix plot\")\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(class_names))\n",
        "        plt.xticks(tick_marks, class_names, rotation=45, ha='right')\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                        fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('forest_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(\"Confusion matrix plot saved as 'forest_confusion_matrix.png'.\")\n",
        "        per_class_f1 = precision_recall_fscore_support(y_true, y_pred, average=None)[2]\n",
        "        print(f\"\\nper Class f1 Scores\")\n",
        "        for i, (class_name, f1_score) in enumerate(zip(class_names, per_class_f1)):\n",
        "            print(f\"{class_name}: {f1_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLzmZT3-p02c"
      },
      "outputs": [],
      "source": [
        "def run_complete_system():\n",
        "    print(\"executing the complete system\")\n",
        "\n",
        "    system = ForestChangeDetectionSystem(\"amazon forest\")\n",
        "    print(\"Starting the process...\")\n",
        "    time_series_data, labels = system.simulate_satellite_data(n_pixels=7500, n_timesteps=48)\n",
        "    change_candidates = system.perform_bfast_analysis()\n",
        "    cnn_features = system.extract_cnn_spatial_features()\n",
        "    ancillary_features = system.extract_ancillary_features()\n",
        "    test_results = system.train_classifier_model()\n",
        "    X_test, y_test, y_pred, class_names, y_pred_proba = test_results\n",
        "\n",
        "\n",
        "    print(\"\\nexecution complete\")\n",
        "\n",
        "    print(f\"\\nFinal performance summary:\")\n",
        "    print(f\"test Accuracy {system.model_metrics['accuracy']:.1%}\")\n",
        "    print(f\"weighted f1 Score {system.model_metrics['f1_score']:.4f}\")\n",
        "    print(f\"number of features used {system.model_metrics['n_features_selected']}\")\n",
        "    print(f\"change rate detected {len(system.change_candidates)/len(system.time_series_ndvi)*100:.1f}%\")\n",
        "    print(f\"probability calibration Applied {'Yes' if system.model_metrics['calibrated'] else 'No'}\")\n",
        "\n",
        "    with open('final_forest_change_system.pkl', 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'system': system,\n",
        "            'metrics': system.model_metrics,\n",
        "            'test_results': (X_test, y_test, y_pred, class_names, y_pred_proba),\n",
        "            'ablation_config': {\n",
        "                'USE_HUBER': USE_HUBER,\n",
        "                'USE_MULTIINDEX': USE_MULTIINDEX,\n",
        "                'USE_SAVGOL': USE_SAVGOL,\n",
        "                'USE_CALIBRATION': USE_CALIBRATION\n",
        "            }\n",
        "        }, f)\n",
        "    print(f\"\\nSystem object saved to 'final_forest_change_system.pkl'.\")\n",
        "    print(f\"Confusion matrix plot saved to 'forest_confusion_matrix.png'.\")\n",
        "    return system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oKX_zS6gr9A0"
      },
      "outputs": [],
      "source": [
        "print(\"starting final system execution...\")\n",
        "system = run_complete_system()\n",
        "print(\"Whole thing is complete\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN95BLKqMsaPARvCW7GCdlP"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
